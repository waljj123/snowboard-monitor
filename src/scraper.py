import requests
from bs4 import BeautifulSoup
import json
import csv
import time
import random
import re
import os
import sys
import logging
from datetime import datetime
from urllib.parse import urljoin

# ç¡®ä¿å¿…è¦çš„ç›®å½•å­˜åœ¨
def setup_directories():
    """åˆ›å»ºå¿…è¦çš„ç›®å½•"""
    directories = ['logs', 'data', 'web/images']
    for directory in directories:
        os.makedirs(directory, exist_ok=True)
        print(f"âœ… åˆ›å»ºç›®å½•: {directory}")

# åˆå§‹åŒ–ç›®å½•
setup_directories()

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/scraper.log', encoding='utf-8'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

class SnowboardsScraper:
    def __init__(self, base_url='https://snowboards.com'):
        self.base_url = base_url
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
        })
        
        # åˆ›å»ºç›®å½•
        self.web_dir = 'web'
        self.data_dir = 'data'
        self.images_dir = os.path.join(self.web_dir, 'images')
        os.makedirs(self.web_dir, exist_ok=True)
        os.makedirs(self.data_dir, exist_ok=True)
        os.makedirs(self.images_dir, exist_ok=True)
        
        # é¢„å®šä¹‰å“ç‰Œåˆ—è¡¨
        self.brands = [
            'Burton', 'Lib Tech', 'Salomon', 'K2', 'Capita', 'Ride', 'Rome',
            'Never Summer', 'Gnu', 'Arbor', 'Bataleon', 'YES', 'Rossignol',
            'Roxy', 'Forum', 'Gilson', 'Public', 'United Shapes', 'WhiteSpace',
            'Nidecker', 'Jones', 'DC', 'Switchback', 'Slash', 'Telos', 'Weston'
        ]

    def get_page(self, page_num=1):
        """è·å–é¡µé¢å†…å®¹"""
        try:
            if page_num == 1:
                url = f'{self.base_url}/products/2672/equipment-snowboards?view=all'
            else:
                url = f'{self.base_url}/products/2672/equipment-snowboards?page={page_num}&view=all'
            
            logger.info(f'ğŸ“„ è·å–é¡µé¢ {page_num}')
            response = self.session.get(url, timeout=20)
            response.raise_for_status()
            
            if len(response.text) < 1000:
                logger.warning('é¡µé¢å†…å®¹è¿‡å°‘')
                return None
                
            logger.info(f'âœ… æˆåŠŸè·å–é¡µé¢ {page_num}')
            return response.text
            
        except Exception as e:
            logger.error(f'âŒ è·å–é¡µé¢å¤±è´¥: {e}')
            return None

    def parse_products(self, html_content):
        """è§£æäº§å“ä¿¡æ¯"""
        if not html_content:
            return []
            
        soup = BeautifulSoup(html_content, 'html.parser')
        products = []
        
        # ä¿å­˜HTMLç”¨äºè°ƒè¯•
        debug_file = os.path.join(self.data_dir, f'debug_{datetime.now().strftime("%Y%m%d_%H%M%S")}.html')
        with open(debug_file, 'w', encoding='utf-8') as f:
            f.write(html_content)
        logger.info(f'ğŸ’¾ ä¿å­˜è°ƒè¯•HTMLåˆ°: {debug_file}')
        
        # å°è¯•å¤šç§é€‰æ‹©å™¨å®šä½äº§å“
        product_selectors = [
            '.product-item', '.product-card', '.product', 
            'div[data-product-id]', '.item', '.grid-item',
            '.tile', '.product-tile', 'li.product',
            'article.product', 'div.product-tile'
        ]
        
        products_found = []
        for selector in product_selectors:
            products_found = soup.select(selector)
            if products_found:
                logger.info(f'ğŸ” ä½¿ç”¨é€‰æ‹©å™¨ "{selector}" æ‰¾åˆ° {len(products_found)} ä¸ªäº§å“')
                break
        
        if not products_found:
            logger.info('å°è¯•é€šç”¨é€‰æ‹©å™¨')
            products_found = soup.find_all(['div', 'li', 'article'], 
                                         class_=lambda x: x and any(word in str(x) for word in ['product', 'item', 'card', 'tile']))
        
        logger.info(f'ğŸ“Š æ‰¾åˆ° {len(products_found)} ä¸ªæ½œåœ¨äº§å“å®¹å™¨')
        
        for i, container in enumerate(products_found[:50]):
            try:
                product = self.extract_product(container)
                if product and product.get('name') and product.get('name') != 'æœªçŸ¥äº§å“':
                    products.append(product)
                    logger.info(f'âœ… æå–äº§å“ {i+1}: {product.get("brand", "æœªçŸ¥")} - {product.get("name")[:30]}...')
            except Exception as e:
                logger.error(f'âŒ è§£æäº§å“ {i+1} å¤±è´¥: {e}')
                continue
        
        return products

    def extract_product(self, container):
        """ä»å®¹å™¨æå–å•ä¸ªäº§å“ä¿¡æ¯"""
        try:
            # è·å–äº§å“åç§°
            name = self.extract_name(container)
            if not name or name == 'æœªçŸ¥äº§å“':
                return None
            
            # è·å–å“ç‰Œ
            brand = self.extract_brand(name, container.get_text())
            
            # è·å–ä»·æ ¼
            price_data = self.extract_price(container)
            
            # è·å–å›¾ç‰‡
            image_url = self.extract_image(container)
            
            # è·å–é“¾æ¥
            product_url = self.extract_url(container)
            
            # ä¸‹è½½å›¾ç‰‡
            image_filename = self.download_image(image_url, brand, name) if image_url else None
            
            product = {
                'id': f'prod_{int(time.time())}_{random.randint(1000, 9999)}',
                'brand': brand,
                'name': name[:200],
                'current_price': price_data.get('current'),
                'original_price': price_data.get('original'),
                'discount': price_data.get('discount'),
                'image_url': image_url,
                'local_image': image_filename,
                'product_url': product_url,
                'category': self.detect_category(name, brand),
                'scraped_at': datetime.now().isoformat(),
                'updated_at': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            }
            
            return product
            
        except Exception as e:
            logger.error(f'æå–äº§å“å¤±è´¥: {e}')
            return None

    def extract_name(self, container):
        """æå–äº§å“åç§°"""
        # å°è¯•å¤šç§é€‰æ‹©å™¨
        name_selectors = [
            '.product-name', '.name', 'h1', 'h2', 'h3', 'h4',
            '.title', '[itemprop="name"]', '.product-title',
            'a.product-name', '.product-link', '.card-title',
            '.product-name a', 'h2 a', '.product__title'
        ]
        
        for selector in name_selectors:
            element = container.select_one(selector)
            if element and element.text.strip():
                name = element.text.strip()
                if len(name) > 3 and not name.lower().startswith(('$', 'from', 'select')):
                    return name
        
        # ä»æ•´ä¸ªå®¹å™¨æ–‡æœ¬ä¸­æå–
        text = container.get_text(strip=True)
        lines = [line.strip() for line in text.split('\n') if line.strip()]
        for line in lines:
            if 10 <= len(line) <= 100:
                if not line.startswith('$') and not any(word in line.lower() for word in ['compare', 'select', 'size', 'color']):
                    return line
        
        return "æœªçŸ¥äº§å“"

    def extract_brand(self, product_name, text):
        """æå–å“ç‰Œ"""
        text_lower = text.lower()
        product_name_lower = product_name.lower()
        
        # ä»é¢„å®šä¹‰å“ç‰Œåˆ—è¡¨åŒ¹é…
        for brand in self.brands:
            if brand.lower() in text_lower or brand.lower() in product_name_lower:
                return brand
        
        # å¸¸è§å“ç‰Œå…³é”®è¯
        brand_keywords = {
            'burton': 'Burton',
            'lib tech': 'Lib Tech',
            'libtech': 'Lib Tech',
            'salomon': 'Salomon',
            'k2': 'K2',
            'capita': 'Capita',
            'ride': 'Ride',
            'rome': 'Rome',
            'never summer': 'Never Summer',
            'gnu': 'Gnu',
            'arbor': 'Arbor',
            'bataleon': 'Bataleon',
            'yes.': 'YES',
            'rossignol': 'Rossignol',
            'roxy': 'Roxy'
        }
        
        for keyword, brand in brand_keywords.items():
            if keyword in text_lower or keyword in product_name_lower:
                return brand
        
        # ä»äº§å“åç§°å¼€å¤´æå–å¯èƒ½çš„å“ç‰Œ
        words = product_name.split()
        if len(words) > 1:
            first_word = words[0]
            if len(first_word) > 1 and first_word[0].isupper():
                return first_word
        
        return "å…¶ä»–å“ç‰Œ"

    def extract_price(self, container):
        """æå–ä»·æ ¼ä¿¡æ¯"""
        text = container.get_text()
        
        # æŸ¥æ‰¾æ‰€æœ‰ä»·æ ¼
        price_pattern = r'\$(\d{1,3}(?:,\d{3})*(?:\.\d{2})?)'
        prices = re.findall(price_pattern, text)
        
        # æ¸…ç†ä»·æ ¼
        price_values = []
        for price in prices:
            try:
                clean_price = price.replace(',', '')
                price_float = float(clean_price)
                price_values.append(price_float)
            except ValueError:
                continue
        
        price_values = sorted(set(price_values))
        price_data = {}
        
        if len(price_values) >= 2:
            price_data['current'] = f"${price_values[0]:.2f}"
            price_data['original'] = f"${price_values[1]:.2f}"
            if price_values[1] > 0:
                discount = (price_values[1] - price_values[0]) / price_values[1] * 100
                price_data['discount'] = f"-{int(discount)}%"
        elif price_values:
            price_data['current'] = f"${price_values[0]:.2f}"
            price_data['original'] = None
            price_data['discount'] = None
        else:
            price_data['current'] = None
            price_data['original'] = None
            price_data['discount'] = None
        
        return price_data

    def extract_image(self, container):
        """æå–å›¾ç‰‡URL"""
        img_selectors = [
            'img[src]', 'img[data-src]', 'img[data-original]',
            '.product-image img', '.main-image img', '.product-img',
            '[data-product-image]', 'source[srcset]', 'img.product-image',
            'img[class*="image"]', 'img[loading="lazy"]'
        ]
        
        for selector in img_selectors:
            img = container.select_one(selector)
            if img:
                src = None
                for attr in ['src', 'data-src', 'data-original', 'srcset', 'data-srcset']:
                    if img.get(attr):
                        src = img.get(attr)
                        break
                
                if src:
                    # å¤„ç†srcset
                    if ' ' in src and ',' in src:
                        src = src.split(',')[0].split(' ')[0]
                    
                    # æ¸…ç†URL
                    src = src.strip()
                    if src.startswith('//'):
                        src = 'https:' + src
                    elif src.startswith('/'):
                        src = urljoin(self.base_url, src)
                    
                    if src and not src.startswith(('data:', 'javascript:')):
                        return src
        
        return None

    def extract_url(self, container):
        """æå–äº§å“é“¾æ¥"""
        link_selectors = ['a[href]', '.product-link', 'a.product-name', 'a[class*="link"]', 'a.product__link']
        
        for selector in link_selectors:
            link = container.select_one(selector)
            if link and link.get('href'):
                href = link.get('href').strip()
                if href and not href.startswith(('#', 'javascript:')):
                    if href.startswith('/'):
                        return urljoin(self.base_url, href)
                    elif href.startswith('http'):
                        return href
        
        return None

    def detect_category(self, name, brand):
        """æ£€æµ‹äº§å“ç±»åˆ«"""
        name_lower = name.lower()
        
        categories = {
            'ç”·å­é›ªæ¿': ['men', "men's", 'ç”·å­', 'ç”·æ¬¾', 'male'],
            'å¥³å­é›ªæ¿': ['women', "women's", 'å¥³å­', 'å¥³æ¬¾', 'female', 'ladies'],
            'å„¿ç«¥é›ªæ¿': ['kid', 'child', 'å„¿ç«¥', 'å°‘å„¿', 'youth', 'junior'],
            'è‡ªç”±å¼é›ªæ¿': ['freestyle', 'park', 'jib', 'twin'],
            'å…¨èƒ½é›ªæ¿': ['all-mountain', 'all mountain', 'freeride'],
            'é‡é›ªé›ªæ¿': ['powder', 'pow', 'backcountry', 'é‡é›ª']
        }
        
        for category, keywords in categories.items():
            if any(keyword in name_lower for keyword in keywords):
                return category
        
        return 'é›ªæ¿'

    def download_image(self, image_url, brand, name):
        """ä¸‹è½½äº§å“å›¾ç‰‡"""
        if not image_url:
            return None
        
        try:
            safe_brand = re.sub(r'[<>:"/\\|?*]', '', brand)[:20]
            safe_name = re.sub(r'[<>:"/\\|?*]', '', name)[:30]
            safe_name = re.sub(r'\s+', '_', safe_name)
            
            ext = 'jpg'
            if '.' in image_url:
                url_ext = image_url.split('.')[-1].lower().split('?')[0]
                if url_ext in ['jpg', 'jpeg', 'png', 'gif', 'webp']:
                    ext = url_ext
            
            filename = f"{safe_brand}_{safe_name}_{int(time.time())%10000}.{ext}"
            filepath = os.path.join(self.images_dir, filename)
            
            if os.path.exists(filepath):
                return filename
            
            logger.info(f'â¬‡ï¸ ä¸‹è½½å›¾ç‰‡: {image_url[:50]}...')
            response = self.session.get(image_url, timeout=15)
            response.raise_for_status()
            
            with open(filepath, 'wb') as f:
                f.write(response.content)
            
            logger.info(f'âœ… å›¾ç‰‡ä¿å­˜: {filename}')
            return filename
            
        except Exception as e:
            logger.error(f'âŒ ä¸‹è½½å›¾ç‰‡å¤±è´¥: {e}')
            return None

    def save_data(self, products):
        """ä¿å­˜æ•°æ®åˆ°JSONå’ŒCSV"""
        if not products:
            logger.warning('âš ï¸ æ²¡æœ‰æ•°æ®å¯ä¿å­˜')
            return None
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # ä¿å­˜JSONæ•°æ®
        json_data = {
            'metadata': {
                'total_products': len(products),
                'unique_brands': len(set(p['brand'] for p in products)),
                'last_updated': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                'source': self.base_url
            },
            'products': products
        }
        
        # ä¿å­˜åˆ°webç›®å½•ç”¨äºGitHub Pages
        json_file = os.path.join(self.web_dir, 'data.json')
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(json_data, f, ensure_ascii=False, indent=2)
        
        # åŒæ—¶ä¿å­˜åˆ°dataç›®å½•å¤‡ä»½
        json_file_backup = os.path.join(self.data_dir, f'snowboards_{timestamp}.json')
        with open(json_file_backup, 'w', encoding='utf-8') as f:
            json.dump(json_data, f, ensure_ascii=False, indent=2)
        
        logger.info(f'ğŸ’¾ ä¿å­˜JSONæ•°æ®: {json_file}')
        
        # ä¿å­˜CSVå¤‡ä»½
        csv_file_backup = os.path.join(self.data_dir, f'snowboards_{timestamp}.csv')
        with open(csv_file_backup, 'w', newline='', encoding='utf-8-sig') as f:
            if products:
                fieldnames = products[0].keys()
                writer = csv.DictWriter(f, fieldnames=fieldnames)
                writer.writeheader()
                writer.writerows(products)
        
        logger.info(f'ğŸ’¾ ä¿å­˜CSVæ•°æ®: {csv_file_backup}')
        
        return {
            'json': json_file,
            'csv': csv_file_backup,
            'count': len(products)
        }

    def scrape_all_pages(self, max_pages=2):
        """çˆ¬å–æ‰€æœ‰é¡µé¢"""
        logger.info('ğŸš€ å¼€å§‹çˆ¬å–é›ªæ¿æ•°æ®...')
        logger.info(f'ğŸ“ æ•°æ®ç›®å½•: {self.data_dir}')
        logger.info(f'ğŸ–¼ï¸ å›¾ç‰‡ç›®å½•: {self.images_dir}')
        
        all_products = []
        
        for page in range(1, max_pages + 1):
            logger.info(f'ğŸ“„ æ­£åœ¨å¤„ç†ç¬¬ {page}/{max_pages} é¡µ')
            
            # è·å–é¡µé¢
            html = self.get_page(page)
            if not html:
                logger.warning(f'âš ï¸ ç¬¬ {page} é¡µè·å–å¤±è´¥')
                if page == 1:
                    logger.error('âŒ ç¬¬ä¸€é¡µè·å–å¤±è´¥')
                    break
                continue
            
            # è§£æäº§å“
            products = self.parse_products(html)
            logger.info(f'âœ… ç¬¬ {page} é¡µæ‰¾åˆ° {len(products)} ä¸ªäº§å“')
            
            all_products.extend(products)
            
            # é¡µé—´å»¶è¿Ÿ
            if page < max_pages and products:
                delay = random.uniform(2, 4)
                logger.info(f'â³ ç­‰å¾… {delay:.1f} ç§’åç»§ç»­...')
                time.sleep(delay)
        
        # å»é‡
        seen = set()
        unique_products = []
        for product in all_products:
            product_key = f"{product.get('brand')}_{product.get('name')}_{product.get('current_price')}"
            if product_key not in seen:
                seen.add(product_key)
                unique_products.append(product)
        
        logger.info(f'ğŸ“Š å»é‡åå‰©ä½™ {len(unique_products)} ä¸ªäº§å“')
        
        if unique_products:
            # ä¿å­˜æ•°æ®
            saved_files = self.save_data(unique_products)
            
            # ç»Ÿè®¡ä¿¡æ¯
            brands = set(p['brand'] for p in unique_products)
            categories = set(p['category'] for p in unique_products)
            
            logger.info('=' * 50)
            logger.info(f'âœ… çˆ¬å–å®Œæˆï¼')
            logger.info(f'ğŸ“¦ æ€»è®¡äº§å“: {len(unique_products)} ä¸ª')
            logger.info(f'ğŸ·ï¸ å“ç‰Œæ•°é‡: {len(brands)} ä¸ª')
            logger.info(f'ğŸ“ ç±»åˆ«æ•°é‡: {len(categories)} ä¸ª')
            logger.info('=' * 50)
            
            return {
                'products': unique_products,
                'files': {
                    'json': saved_files["json"] if saved_files else None,
                    'csv': saved_files["csv"] if saved_files else None
                }
            }
        else:
            logger.error('âŒ æ²¡æœ‰è·å–åˆ°ä»»ä½•äº§å“æ•°æ®')
            return None

def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("ğŸ‚ é›ªæ¿æ•°æ®çˆ¬è™«")
    print("=" * 60)
    
    try:
        # åˆ›å»ºçˆ¬è™«å®ä¾‹
        scraper = SnowboardsScraper()
        
        # çˆ¬å–æ•°æ®
        result = scraper.scrape_all_pages(max_pages=2)
        
        if result:
            products = result['products']
            files = result['files']
            
            print(f"\nâœ… çˆ¬å–å®Œæˆï¼å…±è·å– {len(products)} ä¸ªäº§å“")
            print(f"\nğŸ“ ç”Ÿæˆçš„æ–‡ä»¶:")
            print(f"  ğŸ“„ JSONæ–‡ä»¶: {files.get('json', 'æ— ')}")
            print(f"  ğŸ“Š CSVæ–‡ä»¶: {files.get('csv', 'æ— ')}")
            print(f"  ğŸ–¼ï¸ å›¾ç‰‡ç›®å½•: {scraper.images_dir}/")
            
            # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
            brands = {}
            for product in products:
                brand = product.get('brand', 'æœªçŸ¥å“ç‰Œ')
                brands[brand] = brands.get(brand, 0) + 1
            
            print(f"\nğŸ“ˆ å“ç‰Œç»Ÿè®¡:")
            for brand, count in sorted(brands.items(), key=lambda x: x[1], reverse=True)[:5]:
                print(f"  {brand}: {count} ä¸ªäº§å“")
            
            # æ˜¾ç¤ºå‰å‡ ä¸ªäº§å“ç¤ºä¾‹
            print(f"\nğŸ¯ äº§å“ç¤ºä¾‹ (å‰3ä¸ª):")
            for i, product in enumerate(products[:3]):
                print(f"{i+1}. {product.get('brand')} - {product.get('name')[:40]}...")
                price_info = product.get('current_price', 'ä»·æ ¼å¾…å®š')
                if product.get('discount'):
                    price_info += f" ({product.get('discount')} æŠ˜æ‰£)"
                print(f"   ğŸ’° ä»·æ ¼: {price_info}")
                print(f"   ğŸ·ï¸ ç±»åˆ«: {product.get('category')}")
                print()
        else:
            print("âŒ çˆ¬å–å¤±è´¥ï¼Œæ²¡æœ‰è·å–åˆ°æ•°æ®")
            sys.exit(1)
            
    except KeyboardInterrupt:
        print("\nâ¹ï¸ ç”¨æˆ·ä¸­æ–­ç¨‹åº")
        sys.exit(0)
    except Exception as e:
        logger.error(f"âŒ ç¨‹åºè¿è¡Œå‡ºé”™: {e}", exc_info=True)
        print(f"\nâŒ ç¨‹åºè¿è¡Œå‡ºé”™: {e}")
        sys.exit(1)

if __name__ == '__main__':
    main()