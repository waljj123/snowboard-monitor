name: Daily Snowboard Data Scraper

on:
  schedule:
    - cron: '0 2 * * *'  # æ¯å¤©UTCæ—¶é—´2ç‚¹ï¼ˆåŒ—äº¬æ—¶é—´10ç‚¹ï¼‰
  workflow_dispatch:
  push:
    branches: [ main ]

permissions:
  contents: write
  pages: write
  id-token: write

jobs:
  scrape-data:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      with:
        token: ${{ secrets.GITHUB_TOKEN }}
    
    - name: Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install requests beautifulsoup4 lxml
        
    - name: Create necessary directories
      run: |
        mkdir -p logs data web/images
        
    - name: Run snowboard scraper
      run: |
        python src/scraper.py
        
    - name: Create .nojekyll file
      run: |
        touch web/.nojekyll
        
    - name: Commit and push changes
      run: |
        git config --local user.email "action@github.com"
        git config --local user.name "GitHub Actions"
        git add data/ web/ logs/
        git status
        git diff --staged --quiet || (git commit -m "ğŸ¤– Auto-update: $(date +'%Y-%m-%d %H:%M')" && git push origin main) || echo "æ²¡æœ‰æ›´æ”¹å¯æäº¤"
        
    - name: Deploy to GitHub Pages
      uses: peaceiris/actions-gh-pages@v3
      with:
        github_token: ${{ secrets.GITHUB_TOKEN }}
        publish_dir: ./web
        publish_branch: gh-pages
        force_orphan: true
        
    - name: Upload artifacts
      uses: actions/upload-artifact@v4
      with:
        name: snowboard-data
        path: |
          web/
          data/
          logs/
        retention-days: 7